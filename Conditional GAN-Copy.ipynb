{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToPILImage\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "### load project files\n",
    "import models_cgan as models\n",
    "from models_cgan import weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "\tdataRoot='./posters_resized'\n",
    "\tworkers=12\n",
    "\tbatchSize=128\n",
    "\timageSize=64\n",
    "\tnz=100\n",
    "\tngf=64\n",
    "\tndf=64\n",
    "\tniter=80\n",
    "\tlr=0.0002\n",
    "\tbeta1=0.5\n",
    "\tcuda=True\n",
    "\tngpu=1\n",
    "\tnetG= ''\n",
    "\tnetD= ''\n",
    "\toutDir='./results'\n",
    "\tmodel=1\n",
    "\td_labelSmooth=0.1      # 0.25 from imporved-GANpaper\n",
    "\tn_extra_layers_d=0\n",
    "\tn_extra_layers_g=1     #in the sense that generator should be more powerful\n",
    "\tbinary = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(args.outDir)\n",
    "except OSError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG_1 (\n",
      "  (main): Sequential (\n",
      "    (0): ConvTranspose2d(105, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): LeakyReLU (0.2, inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): LeakyReLU (0.2, inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): LeakyReLU (0.2, inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (11): LeakyReLU (0.2, inplace)\n",
      "    (extra-layers-0.64.conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (extra-layers-0.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (extra-layers-0.64.relu): LeakyReLU (0.2, inplace)\n",
      "    (final_layer.deconv): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final_layer.tanh): Tanh ()\n",
      "  )\n",
      ")\n",
      "_netD_1 (\n",
      "  (main): Sequential (\n",
      "    (0): Conv2d(8, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU (0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU (0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU (0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): LeakyReLU (0.2, inplace)\n",
      "    (final_layers.conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (final_layers.sigmoid): Sigmoid ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "args.manualSeed = random.randint(1,10000) # fix seed, a scalar\n",
    "random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "\n",
    "\n",
    "# In[152]:\n",
    "\n",
    "#nc = 3\n",
    "nc = 8\n",
    "ngpu = args.ngpu\n",
    "nz = args.nz\n",
    "ngf = args.ngf\n",
    "ndf = args.ndf\n",
    "n_extra_d = args.n_extra_layers_d\n",
    "n_extra_g = args.n_extra_layers_g\n",
    "\n",
    "\n",
    "# In[153]:\n",
    "\n",
    "dataset = dset.ImageFolder(\n",
    "    root=args.dataRoot,\n",
    "    transform=transforms.Compose([\n",
    "            transforms.Scale(args.imageSize),\n",
    "            # transforms.CenterCrop(args.imageSize),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)), # bring images to (-1,1)\n",
    "        ])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# In[154]:\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batchSize,\n",
    "                                         shuffle=True, num_workers=args.workers)\n",
    "\n",
    "\n",
    "# In[155]:\n",
    "\n",
    "def concat_channel(images,labels):\n",
    "    new_images = []\n",
    "    for image,label in zip(images,labels):\n",
    "        a = np.zeros([5,64,64])\n",
    "        a[label-1] += 1\n",
    "        new_image = np.concatenate([image.numpy(),a])\n",
    "        \n",
    "        new_images.append(new_image)\n",
    "        \n",
    "    new_images = np.stack(new_images)\n",
    "    return torch.from_numpy(new_images).float()\n",
    "\n",
    "\n",
    "# In[179]:\n",
    "\n",
    "def newconcat(images, labels):\n",
    "    y_onehot = torch.FloatTensor(args.batchSize, 5)\n",
    "    #print (label.unsqueeze(1).size())\n",
    "    try:\n",
    "        y_onehot.zero_().scatter_(1, labels.unsqueeze(1)-1, 1)\n",
    "    except:\n",
    "        y_onehot.zero_().scatter_(1, labels-1, 1)\n",
    "    y_onehot = torch.unsqueeze(torch.unsqueeze(y_onehot, 2),3).expand(args.batchSize, 5, args.imageSize, args.imageSize)\n",
    "\n",
    "    return torch.cat((images, y_onehot),1)\n",
    "\n",
    "\n",
    "# In[192]:\n",
    "\n",
    "def newconcat_noise(images, labels):\n",
    "    y_onehot = torch.FloatTensor(args.batchSize, 5)\n",
    "    #print (label.unsqueeze(1).size())\n",
    "    try:\n",
    "        y_onehot.zero_().scatter_(1, labels.unsqueeze(1)-1, 1)\n",
    "    except:\n",
    "        y_onehot.zero_().scatter_(1, labels-1, 1)\n",
    "    #y_onehot = torch.unsqueeze(torch.unsqueeze(y_onehot, 2),3).expand(args.batchSize, 5, args.imageSize, args.imageSize)\n",
    "\n",
    "    return torch.cat((images, y_onehot),1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load models\n",
    "if args.model == 0:\n",
    "    netG = models._netG_0(ngpu, nz, 3, ngf)\n",
    "    netD = models._netD_0(ngpu, nz, nc, ndf)\n",
    "elif args.model == 1:\n",
    "    netG = models._netG_1(ngpu, 105, 3, ngf, n_extra_g)\n",
    "    netD = models._netD_1(ngpu, 105, nc, ndf, n_extra_d)\n",
    "elif args.model == 2:\n",
    "    netG = models._netG_2(ngpu, nz, 3, ngf)\n",
    "    netD = models._netD_2(ngpu, nz, nc, ndf)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "if args.netG != '':\n",
    "    netG.load_state_dict(torch.load(args.netG))\n",
    "print(netG)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "if args.netD != '':\n",
    "    netD.load_state_dict(torch.load(args.netD))\n",
    "print(netD)\n",
    "\n",
    "\n",
    "# In[219]:\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_MSE = nn.MSELoss()\n",
    "\n",
    "input = torch.FloatTensor(args.batchSize, nc, args.imageSize, args.imageSize)\n",
    "noise = torch.FloatTensor(args.batchSize, nz, 1, 1)\n",
    "if args.binary:\n",
    "    bernoulli_prob = torch.FloatTensor(args.batchSize, nz, 1, 1).fill_(0.5)\n",
    "    fixed_noise = torch.bernoulli(bernoulli_prob)\n",
    "else:\n",
    "    fixed_noise = torch.FloatTensor(args.batchSize, nz, 1, 1).normal_(0, 1)\n",
    "label = torch.FloatTensor(args.batchSize)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "\n",
    "input = Variable(input)\n",
    "label = Variable(label)\n",
    "noise = Variable(noise)\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "\n",
    "# setup argsimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args.lr, betas=(args.beta1, 0.999))\n",
    "\n",
    "\n",
    "# In[220]:\n",
    "\n",
    "for epoch in range(args.niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        start_iter = time.time()\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        images,labels= data[0],data[1]      \n",
    "        images = concat_channel(images,labels)\n",
    "        data = images,labels\n",
    "\n",
    "        \n",
    "        real_cpu = images\n",
    "        #print (real_cpu.size())\n",
    "        batch_size = args.batchSize\n",
    "\n",
    "        input.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "        #print(label)\n",
    "        label.data.resize_(batch_size).fill_(real_label - args.d_labelSmooth) # use smooth label for discriminator\n",
    "        #print( input.size() )\n",
    "        output = netD(input)\n",
    "\n",
    "        #print (output.size())\n",
    "        #print (label.size())\n",
    "        \n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "        \n",
    "        # train with fake\n",
    "        noise.data.resize_(batch_size, nz, 1, 1)\n",
    "        if args.binary:\n",
    "            bernoulli_prob.resize_(noise.data.size())\n",
    "            noise.data.copy_(2*(torch.bernoulli(bernoulli_prob)-0.5))\n",
    "        else:\n",
    "            noise.data.normal_(0, 1)\n",
    "\n",
    "        noise = Variable(newconcat_noise(noise.data,labels))\n",
    "\n",
    "        fake = netG(noise)\n",
    "        fake = Variable(newconcat(fake.data,labels))\n",
    "\n",
    "        \n",
    "        label.data.fill_(fake_label)\n",
    "        output = netD(fake.detach()) # add \".detach()\" to avoid backprop through G\n",
    "\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward() # gradients for fake/real will be accumulated\n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step() # .step() can be called once the gradients are computed\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.data.fill_(real_label) # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward(retain_variables=True) # True if backward through the graph for the second time\n",
    "        if args.model == 2: # with z predictor\n",
    "            errG_z = criterion_MSE(z_prediction, noise)\n",
    "            errG_z.backward()\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        end_iter = time.time()\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f Elapsed %.2f s'\n",
    "              % (epoch, args.niter, i, len(dataloader),\n",
    "                 errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2, end_iter-start_iter))\n",
    "        if i % 100 == 0:\n",
    "            # the first 64 samples from the mini-batch are saved.\n",
    "            real_cpu\n",
    "            vutils.save_image(real_cpu[0:64,:3,:,:],\n",
    "                    '%s/real_samples.png' % args.outDir, nrow=8)\n",
    "            fake = netG(Variable(newconcat_noise(fixed_noise.data,labels)))\n",
    "            vutils.save_image(fake.data[0:64,:3,:,:],\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (args.outDir, epoch), nrow=8)\n",
    "    if epoch % 1 == 0:\n",
    "        # do checkpointing\n",
    "        torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (args.outDir, epoch))\n",
    "        torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (args.outDir, epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
